@startuml aws-hourly-runtime-sequence
!theme plain
skinparam shadowing false

title FOMC Agent - Hourly AWS Runtime Sequence\n(Target behavior: ingest -> enrich -> site data publish)

participant "EventBridge Rule\nFetchScheduleRule\n(rate: FOMC_FETCH_INTERVAL_HOURS)" as EB #E3F2FD
participant "Lambda\nfomc-data-fetcher" as Fetcher #E3F2FD
database "S3\n{prefix}-bls-raw" as BLSRaw #E8F5E9
database "S3\n{prefix}-datausa-raw" as DataRaw #E8F5E9
queue "SQS\n{FOMC_ANALYTICS_QUEUE_NAME}" as Q #FFF3E0
queue "SQS DLQ\n{FOMC_ANALYTICS_DLQ_NAME}" as DLQ #FFEBEE
participant "Lambda\nfomc-analytics-processor" as Analytics #FFF3E0
database "S3\n{prefix}-bls-processed" as BLSProcessed #E8F5E9
database "S3\n{prefix}-datausa-processed" as DataProcessed #E8F5E9
participant "Lambda\nfomc-site-publisher" as Publisher #F3E5F5
database "S3\n{prefix}-site\n(site/data/*.json)" as SiteBucket #F3E5F5
participant "CloudFront Distribution" as CF #F3E5F5
participant "CloudWatch Logs + Metrics" as CW #F5F5F5
participant "BLS LABSTAT\ndownload.bls.gov" as BLS #F5F5F5
participant "DataUSA API\napi.datausa.io" as DataUSA #F5F5F5
actor "Browser User" as User

== Hourly trigger ==
EB -> Fetcher : invoke every FOMC_FETCH_INTERVAL_HOURS hour(s)
activate Fetcher
Fetcher -> BLS : sync changed BLS files
BLS --> Fetcher : files + modified timestamps
Fetcher -> BLSRaw : put raw files + _sync_state
Fetcher -> DataUSA : fetch configured datasets
DataUSA --> Fetcher : JSON payloads
Fetcher -> DataRaw : put *.json + _sync_state
Fetcher -> CW : write fetch logs + metrics
deactivate Fetcher

== Raw-to-enriched processing ==
DataRaw -> Q : S3:ObjectCreated (*.json)
Q -> Analytics : event source mapping (batch=1)
activate Analytics
Analytics -> BLSRaw : read BLS input
Analytics -> DataRaw : read DataUSA input
Analytics -> BLSProcessed : put processed/bls/pr.data.0.Current.csv
Analytics -> DataProcessed : put processed/datausa/population.csv
Analytics -> DataProcessed : put processed/datausa/commute_time.csv
Analytics -> DataProcessed : put processed/datausa/citizenship.csv
Analytics -> Publisher : invoke site-data refresh step
Analytics -> CW : write processing logs + metrics
deactivate Analytics

== Site data publish ==
activate Publisher
Publisher -> BLSProcessed : read processed datasets
Publisher -> DataProcessed : read processed datasets
Publisher -> SiteBucket : put site/data/timeseries.json
Publisher -> SiteBucket : put site/data/productivity_vs_compensation.json
Publisher -> SiteBucket : put site/data/unemployment_vs_commute_time.json
Publisher -> SiteBucket : put site/data/participation_vs_noncitizen_share.json
Publisher -> SiteBucket : put site/data/bls_timeline.json + pipeline_status.json
Publisher -> CF : invalidate /data/* + /charts.html + /timeline.html
Publisher -> CW : write publish logs + metrics
deactivate Publisher

== User reads refreshed charts ==
User -> CF : GET /index.html + /data/*.json
CF -> SiteBucket : origin fetch
SiteBucket --> CF : latest static assets + fresh chart JSON
CF --> User : refreshed static website with fresh charts

== Failure path ==
alt analytics or publish step fails repeatedly
  Q -> DLQ : move message after maxReceiveCount=3
  DLQ -> CW : alarmable backlog/error signal
end

@enduml
